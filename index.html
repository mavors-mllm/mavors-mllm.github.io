<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/mavors.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Inserted image above the title -->
  <div class="has-text-centered" style="margin-bottom: 0px;"></div>
    <img src="static/images/mavors.png" alt="Cover Image" style="max-width: 25%; height: auto; margin: 0 auto; display: block;">
  </div>
  <title>Mavors: Multi-granularity Video Representation for Multimodal Large Language Model</title>
  <link rel="icon" type="image/x-icon" href="static/images/mavors.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<style>
  .tight-figure {
    margin-bottom: 0; /* 去除图像下方空隙 */
    margin-top: 0; /* 去除图像上方空隙 */
    margin-left: auto;
    margin-right: auto;
    display: block;
  }

  .tight-text {
    margin-top: 0.5rem; /* 缩小图像和段落之间的空隙 */
  }

  .is-80-percent {
    width: 110%;
    margin-left: auto;
    margin-right: auto;
    padding-left: 1rem;
    padding-right: 1rem;
  }

  .is-small-padding {
  padding-top: 0rem;
  padding-bottom: 0rem;
  margin-top: 0rem;
  margin-bottom: 0rem;
}
</style>


  <section class="section is-small-padding">
    <div class="hero-body">
      <div class="container"><style></style>
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Mavors: Multi-granularity Video Representation for Multimodal Large Language Model</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                Yang Shi</a><sup style="color: #ff613e;">1</sup><sup>,</sup></sup><sup style="color: #33A1FF;">2</sup><sup>,*†</sup>,</span>
                <span class="author-block">
                  Jiaheng Liu</a><sup style="color: #067820;">3</sup><sup>*</sup>,</span>
                  <span class="author-block">
                    Yushuo Guan</a><sup style="color: #33A1FF">2</sup><sup>*</sup>,</span>
                    <span class="author-block">
                      Zhenhua Wu</a><sup style="color: #33A1FF;">2</sup>,</span>
                      <span class="author-block">
                        Yuanxing Zhang</a><sup style="color: #33A1FF;">2</sup><sup>‡</sup>,</span>
                        <span class="author-block">
                            Zihao Wang</a><sup style="color: #33A1FF;">2</sup>,</span><br>
                          <span class="author-block"></span>
                            Weihong Lin</a><sup style="color: #33A1FF;">2</sup>,</span>
                            <span class="author-block">
                              Jingyun Hua</a><sup style="color: #33A1FF;">2</sup>,</span>
                              <span class="author-block">
                                Zekun Wang</a><sup style="color: #33A1FF;">2</sup>,</span>
                                <span class="author-block">
                                  Xinlong Chen</a><sup style="color: #e7bc3b;">4</sup>,</span>
                                  <span class="author-block">
                                    Bohan Zeng</a><sup style="color: #ff613e;">1</sup>,</span>
                                    <span class="author-block">
                                      Wentao Zhang</a><sup style="color: #ff613e;">1</sup>,</span><br>
                                      <span class="author-block">
                                        Fuzheng Zhang</a><sup style="color: #33A1FF;">2</sup>,</span>
                                        <span class="author-block">
                                          Wenjing Yang</a>,
                                          <span class="author-block">
                                            Di Zhang</a><sup style="color: #33A1FF;">2</sup>
            </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup style="color: #ff613e;">1</sup>Peking University,</span>
              <span class="author-block"><sup style="color: #33A1FF;">2</sup>Kuaishou Technology,</span>
              <span class="author-block"><sup style="color: #067820;">3</sup>Nanjing University,</span>
              <span class="author-block"><sup style="color: #e7bc3b;">4</sup>CASIA</span><br>
                    <span class="eql-cntrb"><small><sup>*</sup>Equal Contribution</small></span>
                    <span class="eql-cntrb"><small><sup>†</sup>Work done during an internship at Kuaishou Technology</small></span>
                    <span class="eql-cntrb"><small><sup>‡</sup>Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <!-- <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span> -->

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <!-- <span class="link-block">
                    <a href="https://github.com/YOUR REPO HERE" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span> -->

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span> -->
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section is-small-padding">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="section is-80-percent">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Long-context video understanding in multimodal large language models (MLLMs) faces a critical challenge: balancing computational efficiency with the retention of fine-grained spatio-temporal patterns. Existing approaches (e.g., sparse sampling, dense sampling with low resolution, and token compression) suffer from significant information loss in temporal dynamics, spatial details, or subtle interactions, particularly in videos with complex motion or varying resolutions. To address this, we propose <strong>Mavors</strong>, a novel framework that introduces <strong>M</strong>ulti-gr<strong>a</strong>nularity <strong>v</strong>ide<strong>o</strong> <strong>r</strong>epre<strong>s</strong>entation for holistic long-video modeling. Specifically, Mavors directly encodes raw video content into latent representations through two core components: 1) an <strong>Intra-chunk Vision Encoder (IVE)</strong> that preserves high-resolution spatial features via 3D convolutions and Vision Transformers, and 2) an <strong>Inter-chunk Feature Aggregator (IFA)</strong> that establishes temporal coherence across chunks using transformer-based dependency modeling with chunk-level rotary position encodings. Moreover, the framework unifies image and video understanding by treating images as single-frame videos via sub-image decomposition. Experiments across diverse benchmarks demonstrate Mavors' superiority in maintaining both spatial fidelity and temporal continuity, significantly outperforming existing methods in tasks requiring fine-grained spatio-temporal reasoning.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper Teaser -->
<section class="section is-small-padding">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="section is-80-percent">
        <h2 class="title is-3">Teaser</h2>

        <figure class="is-tight-figure">
            <img src="static/images/intro_fig.png" alt="Teaser Image" style="max-width: 100%; height: auto;">
        </figure>
        <p class="tight-text" style="text-align: left;">
                Existing video MLLMs struggle to balance computational efficiency with fine-grained spatial-temporal understanding. Sparse sampling drops crucial temporal cues, dense sampling with low-resolution lose spatial details, and token compression harms subtle motion and object interactions. To address this, we propose <strong>Mavors</strong>, a method that extracts Multi-granularity Video Representations, enabling MLLMs to retain spatial fidelity and temporal coherence across diverse video scenarios.
        </p>
      </div>
    </div>
  </div>
</section>
<!-- End paper teaser -->

<!-- Model Architecture -->
<section class="section is-small-padding">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="section is-80-percent">
        <h2 class="title is-3">Model Architecture</h2>
        <figure class="tight-figure">
          <img src="static/images/model_arch3.png" alt="Model Architecture" style="max-width: 100%; height: auto;">
        </figure>
      </div>
    </div>
  </div>
</section>
<!-- End model architecture -->

<!-- Benchmark Performance -->

<!-- Benchmark Performance -->

<!--BibTex citation -</section>->

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>